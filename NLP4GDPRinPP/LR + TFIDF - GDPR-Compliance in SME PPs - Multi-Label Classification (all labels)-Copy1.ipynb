{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='background :yellow' > Assessing SMEs' GDPR-compliance Through Privacy Policies: A Machine Learning Approach </span>\n",
    "<span style='background :yellow' >\n",
    "The goal of this research is to explore--using natural language processing and machine learning techniques--how organisations differ in their approach towards GDPR-compliance. We intend to do this by assessing privacy policies on their *focus* (rather than completeness) of the GDPR user rights and explore whether there is a correlation with the corresponding organisation's meta-data (e.g., country, service, data-driven). This will give us insight in organisations' interpretation of the GDPR (e.g., stressing a specific part) and factors (e.g., company size) that contribute to this particular interpretation.\n",
    "</span>\n",
    "\n",
    "---\n",
    "## DATASET ##\n",
    "This manually labeled set comprises 250 individual policies, containing over 18,300 natural sentences. For legal reasons, we have anonymized the data set, e.g. we have scrambled all num- bers and substituted names, email addresses, companies and URLs with generic replacements (e.g. ‘company 42645’). <br>\n",
    "Source: __On GDPR Compliance of Companies’ Privacy Policies__ _by Müller et al._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five GDPR requirements chosen to evaluate privacy policy compliance:\n",
    "\n",
    "|No.|\tCategory|Required content in privacy policy|\n",
    "|---|---|---|\n",
    "|1| DPO | Contact details for the data protection officer or equivalent |\n",
    "|2| Purpose | Disclosure of the purpose for which personal data is or is not used for |\n",
    "|3| Acquired data | Disclosure that personal data is or is not collected, and/or which data is collected |\n",
    "|4| Data sharing | Disclosure if 3rd parties can or cannot access a user’s personal data |\n",
    "|5| Rights | Disclosure of the user’s right to rectify or erase personal data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import labeled PPs (18.397 sentence snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Let's load the training data from a csv file\n",
    "dataset = pandas.read_csv('data/PP/GDPR.csv', sep='\\t', encoding='utf-8')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore balance of dataset\n",
    "Source: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categ = list(dataset.columns)[1:] # select all except 'text' column\n",
    "\n",
    "counts = []\n",
    "for column in categ:\n",
    "#     print(dataset[column].value_counts())\n",
    "    tmp_count = dataset[column].value_counts()\n",
    "    # make a list of tuples that contain column name and number of pos labeled sentences     \n",
    "    counts.append((column, tmp_count[1]))\n",
    "\n",
    "df_stats = pandas.DataFrame(counts, columns=['GDPR_criteria', 'number_of_pos_sen'])\n",
    "\n",
    "df_stats.plot(x='GDPR_criteria', y='number_of_pos_sen', kind='bar', legend=False, grid=True, figsize=(8, 5))\n",
    "plt.title(\"Number of positively labeled sentences per category\")\n",
    "plt.ylabel('# of Occurrences (of a total of 18.397)', fontsize=12)\n",
    "plt.xlabel('GDPR Assessment Criteria', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the number of multi-labeled sentences?\n",
    "Source: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "rowsums = dataset.iloc[:,2:].sum(axis=1)\n",
    "x=rowsums.value_counts()\n",
    "#plot\n",
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(x.index, x.values)\n",
    "plt.title(\"Multiple GDPR criteria per sentence\")\n",
    "plt.ylabel('# of Occurrences (of a total of 18.397)', fontsize=12)\n",
    "plt.xlabel('# of GDPR criteria', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The vast majority of the sentences is not labeled at all (almost 16.000)\n",
    "\n",
    "#### Class imbalance, possible solutions:\n",
    "- oversampling minority class\n",
    "    - Resample function from scikit-learn packaged: randomly duplicate examples in the minority class.\n",
    "    - generating synthetic samples using SMOTE functionality in Imblearn package\n",
    "- undersampling majority class\n",
    "\n",
    "##### Oversampling should be done on the training set only:\n",
    "In class imbalance settings, artificially balancing the test/validation set does not make any sense: these sets must remain realistic, i.e. you want to test your classifier performance in the real world setting, where, say, the negative class will include the 99% of the samples, in order to see how well your model will do in predicting the 1% positive class of interest without too many false positives. Artificially inflating the minority class or reducing the majority one will lead to performance metrics that are unrealistic, bearing no real relation to the real world problem you are trying to solve.\n",
    "\n",
    "Max Kuhn, creator of the caret R package and co-author of the (highly recommended) Applied Predictive Modelling textbook, in Chapter 11: Subsampling For Class Imbalances of the caret ebook:\n",
    "\n",
    "__You would never want to artificially balance the test set; its class frequencies should be in-line with what one would see “in the wild”.__\n",
    "\n",
    "Re-balancing makes sense only in the training set, so as to prevent the classifier from simply and naively classifying all instances as negative for a perceived accuracy of 99%.\n",
    "\n",
    "Hence, you can rest assured that in the setting you describe the rebalancing takes action only for the training set/folds._\n",
    "\n",
    "sources: \n",
    "- https://imbalanced-learn.org/stable/over_sampling.html\n",
    "- https://stackoverflow.com/questions/48805063/balance-classes-in-cross-validation/48810493#48810493\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "      \n",
    "def oversample_data(x, y):    \n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    x_over, y_over = oversample.fit_resample(x, y)\n",
    "    return x_over, y_over\n",
    "\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocessing(pps):\n",
    "#     tokenizer = nlp.tokenizer\n",
    "    # tokenize sentences\n",
    "    tokenized_sent = [sent.split() for sent in pps]\n",
    "    \n",
    "    # remove punctuation\n",
    "    tokenized_sent = [[re.sub('[,’\\'\\.!?&“”():*_;\"]', '', y) for y in x] for x in tokenized_sent]\n",
    "    \n",
    "    # remove words with numbers in them\n",
    "    tokenized_sent = [[y for y in x if not any(c.isdigit() for c in y)] for x in tokenized_sent]\n",
    "    \n",
    "    # remove stopwords    \n",
    "    tokenized_sent_clean = [[y for y in x if y not in stopwords.words('english')] for x in tokenized_sent]\n",
    "    \n",
    "    # from nltk.stem import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    tokenized_sent_clean = [[porter.stem(y) for y in x] for x in tokenized_sent_clean]\n",
    "    \n",
    "    detokenized_pps = []\n",
    "    for i in range(len(tokenized_sent_clean)):\n",
    "        t = ' '.join(tokenized_sent_clean[i])\n",
    "        detokenized_pps.append(t) \n",
    "    \n",
    "    return detokenized_pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before preprocessing: \")\n",
    "print(pps[0:3])\n",
    "\n",
    "print(\"Post preprocessing: \")\n",
    "print(preprocessing(pps[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "<img src=\"img/tfidfformula.png\">\n",
    "\n",
    "\n",
    "#### TF-IDF Vectorizer\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "- fit: ...\n",
    "- transform: ...\n",
    "- fit transform: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initalise the vectorizer \n",
    "# if we remove the stopwords here, the coef list is larger than vectorizer.get_feature_names()\n",
    "# vectorizer = TfidfVectorizer(min_df = 2,  stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf = vectorizer.fit_transform(pps_n)\n",
    "\n",
    "#compressed sparse row matrix: list of rows\n",
    "type(tfidf)\n",
    "\n",
    "# dense array: easier to work with\n",
    "tfidf = tfidf.toarray()\n",
    "type(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore\n",
    "print(tfidf.shape)\n",
    "print(len(y_purpose))\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "print(len(words))\n",
    "words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Classification: Logistic Regression\n",
    "\n",
    "For classification tasks, Logistic regression models the probabability of an event occurring (e.g., \"DPO\", \"Purpose\") depending on the values of the independent variables, which are categorical (in our case even binary: \"DPO\" is 1 or 0).\n",
    "\n",
    "We know that z is the weighted sum of the evidence for the class (probability of the class occurring).<br>\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "z = c_0+c_1*𝑥_1+c_2*𝑥_2+...+c_𝑛*𝑥_𝑛\n",
    "\\end{align}\n",
    "\n",
    "The larger the weight the greater impact the given feature has on the final decision:<br>\n",
    "- large positive values indicate a positive impact (for the event to occur)\n",
    "- large negative values indicate a negative impact (for the event not to occur)\n",
    "\n",
    "Z value is between -∞ and +∞. \n",
    "Therefore we apply the sigmoid (or logistic function) to this value to obtain prob. values between 0 and 1.\n",
    "The final probability scores let the model predict the label. If the prob of \"Red\" is higher than all other labels, the prediction will be \"Red\".\n",
    "\n",
    "More info: https://machinelearningmastery.com/logistic-regression-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize parameters\n",
    "___max_df___ float in range [0.0, 1.0] or int, default=1.0, is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "- max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "- max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "___min_df___ float in range [0.0, 1.0] or int, default=1, is used for removing terms that appear too infrequently. For example:\n",
    "- min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "- min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "__ngram_range___ tuple (min_n, max_n), default=(1, 1)\n",
    "- The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
    "\n",
    "___max_features___ int, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "Source:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "\n",
    "# 'UR_explicitly_mentioned' weggelaten\n",
    "# categories = ['Purpose']\n",
    "prep_dataset = preprocessing(pps = dataset['Text'].to_list())\n",
    "\n",
    "\n",
    "for i, category in enumerate(categ):\n",
    "    tfidf_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "    # increase processing time in a combinatorial way\n",
    "    parameters = {\n",
    "#         'tfidf__min_df': (.05, .1, .15, .2), #best solution: (.75)\n",
    "#         'tfidf__max_df': (0.75, .85), #best solution: (1.)\n",
    "#         'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "        'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)), #best solution: (1,2)\n",
    "#         'dt__max_depth': np.arange(3,10)\n",
    "    }\n",
    "\n",
    "    y = dataset[category].to_list()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(prep_dataset, y, test_size=0.1)\n",
    "    grid_search = GridSearchCV(tfidf_pipeline, parameters)\n",
    "\n",
    "    print(\"Performing grid search for label: {}\".format(category))\n",
    "    print(\"tf-idf pipeline:\", [name for name, _ in tfidf_pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use optimized params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#In sklearn, all machine learning models are implemented as Python classes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "prep_dataset = preprocessing(pps = dataset['Text'].to_list())\n",
    "\n",
    "params = [\n",
    "    (1,1),\n",
    "    (1,3),\n",
    "    (1,1),\n",
    "    (1,2),\n",
    "    (1,1)\n",
    "]\n",
    "\n",
    "for i, category in enumerate(categ):\n",
    "# for i, category in enumerate([\"DPO\"]):\n",
    "    print(\"Label in progress:\" + category)\n",
    "    print()\n",
    "    \n",
    "    # initalise the vectorizer \n",
    "#     vectorizer = TfidfVectorizer(max_df = .75, min_df = .05, ngram_range = (1,2))\n",
    "    vectorizer = TfidfVectorizer(ngram_range = params[i])\n",
    "    print(\"Ngram:\", params[i])\n",
    "\n",
    "    tfidf = vectorizer.fit_transform(prep_dataset)\n",
    "\n",
    "    # dense array: easier to work with\n",
    "#     tfidf = tfidf.toarray()\n",
    "\n",
    "    x = tfidf\n",
    "    y = dataset[category].to_list()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "    \n",
    "    x_train_over, y_train_over = oversample_data(x_train, y_train)\n",
    "\n",
    "    # Make an instance of the Model\n",
    "    # all parameters not specified are set to their defaults\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    # Train the model on the data, storing the information learned from the data\n",
    "    # Model is learning the relationship between digits (x_train) and labels (y_train)\n",
    "    lr.fit(x_train_over, y_train_over)\n",
    "\n",
    "    # Let's see what are the possible labels to predict (and in which order they are stored)\n",
    "    print(lr.classes_)\n",
    "\n",
    "    # We can get additional information about all the parameters used with LogReg model\n",
    "    print(lr.get_params())\n",
    "\n",
    "    y_pred = lr.predict(x_test)\n",
    "    \n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    print()\n",
    "    print(\"Most important features:\")\n",
    "    for label, coefs, intercept in zip(lr.classes_, lr.coef_, lr.intercept_):\n",
    "        print(label)\n",
    "        sort_zipped_list = sorted(zip(words, coefs), key = lambda x: x[1], reverse = True) \n",
    "        for t, c in list(sort_zipped_list)[:10]:\n",
    "            print(t, c)\n",
    "        print(\"...\")\n",
    "        print(\"INTERCEPT:\" +str(intercept))\n",
    "        print(\"...\")\n",
    "        for t, c in list(sort_zipped_list)[-10:]:\n",
    "            print(t, c)\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    print()\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print()\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"TFIDF ROC_AUC Score\", roc_auc_score(y_test, lr.predict_proba(x_test)[:,1]))\n",
    "    \n",
    "    print(\"----------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict & Evaluate\n",
    "\n",
    "### Accuracy ### \n",
    "For label X, precision is the number of correctly predicted labels divided by all labels<br>\n",
    "\n",
    "\\begin{align}\n",
    "Precision(p) = \\frac{correctly\\ predicted\\ as\\ label\\ A}{all\\ predictions\\ made} = \\frac{true\\ positives\\ +\\ true\\ negatives}{true\\ positives\\ +\\ false\\ positives\\ +\\ true\\ negatives\\ +\\ false\\ negatives} \\\\\n",
    "\\end{align}\n",
    "\n",
    "true positive = correctly predicted as label A<br>\n",
    "false positive = incorrectly predicted as label A<br>\n",
    "true negative = correctly predicted as not label A<br>\n",
    "false negative = predicted as another label, whereas it is actually label A\n",
    "\n",
    "- ___Is not very helpful in case of class imbalance (classifying everything to the majority class will result in this case in a good accuracy)___\n",
    "\n",
    "\n",
    "### Precision ### \n",
    "For label X, precision is the number of correctly predicted labels __out of all predicted labels__ (for the actual label X) (What percent of the predicted labels are correct? The focus is on predictions.).<br>\n",
    "\n",
    "\\begin{align}\n",
    "Precision(p) = \\frac{correctly\\ predicted\\ as\\ label\\ A}{all\\ predictions\\ made\\ as\\ label\\ A} = \\frac{true\\ positives}{true\\ positives\\ +\\ false\\ positives} \\\\\n",
    "\\end{align}\n",
    "\n",
    "true positive = correctly predicted as label A<br>\n",
    "false positive = incorrectly predicted as label A<br>\n",
    "\n",
    "### Recall ### \n",
    "For label X, recall is the number of correctly predicted labels (same as above) __out of the number of actual labels A__ (Out of all actual label A's, what percent of them did the model predict correctly? The focus is on actual labels.).<br>\n",
    "In other words: r = true positives / (true positives + false negatives)\n",
    "\n",
    "\\begin{align}\n",
    "Recall(r) = \\frac{correctly\\ predicted\\ as\\ label\\ A}{all\\ actual\\ items\\ with\\ label\\ A} = \\frac{true\\ positives}{true\\ positives\\ +\\ false\\ negatives} \\\\\n",
    "\\end{align}\n",
    "\n",
    "true positive = correctly predicted as label A<br>\n",
    "false negative = predicted as another label, whereas it is actually label A\n",
    "\n",
    "### F1 Score ###\n",
    "Ok so precision and recall measures the performance of a model from two different perspectives.\n",
    "We can combine the two measures to get a single, balanced score, which is also called __F1 score__.\n",
    "Obtaining a single score is often easier to compare different models.\n",
    "\n",
    "\\begin{align}\n",
    "F1 = 2 * \\frac{Precision * Recall}{Precision + Recall} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = lr.predict(x_test)\n",
    "# len words = 4706 - ngram(1,2)\n",
    "# len words = 4706 - ngram(1,1)\n",
    "print(len(words))\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# roc=roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])\n",
    "# print(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate coefficients and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 10 terms that have the largest weight (coefficients)    \n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "for label, coefs, intercept in zip(lr.classes_, lr.coef_, lr.intercept_):\n",
    "    print(label)\n",
    "    sort_zipped_list = sorted(zip(words, coefs), key = lambda x: x[1], reverse = True) \n",
    "    for t, c in list(sort_zipped_list)[:100]:\n",
    "        print(t, c)\n",
    "    print(\"INTERCEPT:\" +str(intercept))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Xtesting = ['this is the first sentence', 'second sentence', 'third sentence']\n",
    "\n",
    "tfidf_test = TfidfVectorizer(stop_words=None, ngram_range=(2, 2))\n",
    "\n",
    "sps = tfidf_test.fit_transform(Xtesting)\n",
    "print(len(tfidf_test.get_feature_names()[0]))\n",
    "print((tfidf_test.get_feature_names()[0]))\n",
    "# ['ab', 'bc', 'cd', 'de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Let's load the training data from a csv file\n",
    "# train_set = pandas.read_csv('./test.csv', sep='\\t', encoding='utf-8')\n",
    "# train_colors = train_set['Color'].to_list()\n",
    "\n",
    "# Get a dictionary of unique items with their counts\n",
    "# print(Counter(train_colors))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Model|\tPredicted: No|Predicted: Yes|\n",
    "|---|---|---|\n",
    "|Actual: No| 1731 | 3 |\n",
    "|Actual: Yes| 82 | 24 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report\n",
    "- __macro avg__: Calculate precision, recall and f1 metrics for each label, and find their average. This does not take label imbalance into account: f1 scores are averaged (with equal weights)\n",
    "- __weighted avg__: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \"macro\" to account for label imbalance (it can result in an F-score that is not between precision and recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LABEL: DPO\n",
    "___1. Ngram (1,1), preprocessing(+ remove words with digits)*___\n",
    "<img src=\"img/dpo-ngram11.png\">\n",
    "\n",
    "2. Ngram (1,2), preprocessing(+ remove words with digits) <br>\n",
    "<img src=\"img/dpo-ngram12.png\">\n",
    "\n",
    "3. Ngram (1,1), preprocessing(all + remove words with digits), oversampling <br>\n",
    "<img src=\"img/dpo-ngram11-oversampling.png\" height=500>\n",
    "\n",
    "#### LABEL: PURPOSE\n",
    "1. Ngram (1,1), preprocessing(+ remove words with digits)\n",
    "<img src=\"img/purpose-ngram11.png\">\n",
    "\n",
    "___2. Ngram (1,3), preprocessing(+ remove words with digits)*___<br>\n",
    "<img src=\"img/purpose-ngram13.png\">\n",
    "\n",
    "3. Ngram (1,3), preprocessing(all + remove words with digits), oversampling <br>\n",
    "<img src=\"img/purpose-ngram13-oversampling.png\" height=500>\n",
    "\n",
    "#### LABEL: ACQUIRED DATA\n",
    "___1. Ngram (1,1), preprocessing(+ remove words with digits)___\n",
    "<img src=\"img/acquireddata-ngram11.png\">\n",
    "\n",
    "2. Ngram (1,2), preprocessing(+ remove words with digits)<br>\n",
    "<img src=\"img/acquireddata-ngram12.png\">\n",
    "\n",
    "3. Ngram (1,2), preprocessing(all + remove words with digits), oversampling <br>\n",
    "<img src=\"img/acquireddata-ngram11-oversampling.png\" height=500>\n",
    "\n",
    "#### LABEL: DATA SHARING\n",
    "1. Ngram (1,1), preprocessing(all + remove words with digits)\n",
    "<img src=\"img/datasharing-ngram11.png\">\n",
    "\n",
    "___2. Ngram (1,2), preprocessing(all + remove words with digits)*___<br>\n",
    "<img src=\"img/datasharing-ngram12.png\">\n",
    "\n",
    "3. Ngram (1,2), preprocessing(all + remove words with digits), oversampling <br>\n",
    "<img src=\"img/datasharing-ngram12-oversampling.png\" height=500>\n",
    "\n",
    "#### LABEL: RIGHTS\n",
    "___1. Ngram (1,1), preprocessing(+ remove words with digits)*___\n",
    "<img src=\"img/rights-ngram11.png\">\n",
    "\n",
    "2. Ngram (1,2), preprocessing(all + remove words with digits)<br>\n",
    "<img src=\"img/rights-ngram12.png\">\n",
    "\n",
    "3. Ngram (1,1), preprocessing(all + remove words with digits), oversampling <br>\n",
    "<img src=\"img/rights-ngram11-oversampling.png\" height=500>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
