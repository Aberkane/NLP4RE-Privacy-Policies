{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Policies: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "import nltk\n",
    "import os.path\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import socket\n",
    "import pydotplus\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# from six import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import URLError, HTTPError\n",
    "from http.client import IncompleteRead\n",
    "from IPython.display import Image\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that converts list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(s):\n",
    "\t# initialize an empty string\n",
    "\tstr1 = \"\"\n",
    "\t\n",
    "\t# traverse in the string\n",
    "\tfor ele in s:\n",
    "\t\tstr1 += ele\n",
    "\t\n",
    "\t# return string\n",
    "\treturn str1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to scrape a privacy policy from a given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_policy_url(policy_url):\n",
    "\tparsed_policy = \"\"\n",
    "\ttry:\n",
    "\t\tarticle = Article(policy_url)\n",
    "\t\tarticle.download()  # Downloads the link’s HTML content\n",
    "\t\tarticle.parse()  # Parse the article\n",
    "\t\tarticle.nlp()\n",
    "\t\t# print(policy_url)\n",
    "\t\t# print(article.text)\n",
    "\t\tparsed_policy = article\n",
    "\t# except article.ArticleException as ae:\n",
    "\t# print(ae)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\t# avoid too many requests error from Google\n",
    "\t\n",
    "\treturn parsed_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to find the relevant paragraph of a privacy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_paragraph(policy_url):\n",
    "\t# policy_url = \"https://irs.gov/privacy-disclosure/report-phishing\"\n",
    "\t\n",
    "\tpre_dataframe = [[]]\n",
    "\tpotential_paragraph = \"\"\n",
    "\theaders = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "\t                         \"Chrome/41.0.2228.0 Safari/537.3\"}\n",
    "\t\n",
    "\t# header = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:32.0) Gecko/20100101 Firefox/32.0', }\n",
    "\t\n",
    "\ttry:\n",
    "\t\treq = Request(policy_url, headers=headers)\n",
    "\t\thtml = urlopen(req, timeout=1)\n",
    "\t\t\n",
    "\t\t# fill first element of the list (first cell of row) with the relevant website title\n",
    "\t\t# policy_list.append(urlparse(policy.url).netloc + \".txt\")\n",
    "\t\t# html = urlopen('https://automattic.com/privacy/')\n",
    "\t\t\n",
    "\t\tbs = BeautifulSoup(html, \"html.parser\")\n",
    "\t\t# bs = BeautifulSoup(req.text, \"lxml\")\n",
    "\t\t\n",
    "\t\t# 6 levels of HTML-headers\n",
    "\t\t# titles = bs.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\t\t# print('List all the header tags :', *titles, sep='\\n\\n')\n",
    "\t\t\n",
    "\t\tfor header in bs.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "\t\t\t# print(header.text.strip())\n",
    "\t\t\tif (\"right\" or \"rights\") in header.text.strip().lower():\n",
    "\t\t\t\t\n",
    "\t\t\t\tnextNode = header\n",
    "\t\t\t\twhile True:\n",
    "\t\t\t\t\tnextNode = nextNode.nextSibling\n",
    "\t\t\t\t\tif nextNode is None:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tif isinstance(nextNode, NavigableString):\n",
    "\t\t\t\t\t\t# print(nextNode.strip())\n",
    "\t\t\t\t\t\tpotential_paragraph = potential_paragraph + \" \" + nextNode.strip()\n",
    "\t\t\t\t\tif isinstance(nextNode, Tag):\n",
    "\t\t\t\t\t\tif nextNode.name == \"h2\":\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t# print(nextNode.get_text(strip=True).strip())\n",
    "\t\t\t\t\t\tpotential_paragraph = potential_paragraph + \" \" + nextNode.get_text(strip=True).strip()\n",
    "\t\n",
    "\t# policy_list.append(header.text.strip())\n",
    "\t# print(\"*** *** *** END\")\n",
    "\t# pre_dataframe.append(policy_list)\n",
    "\texcept HTTPError as e:\n",
    "\t\t# do something\n",
    "\t\tprint('Error code: ', e.code)\n",
    "\t\tpass\n",
    "\texcept URLError as e:\n",
    "\t\t# do something\n",
    "\t\tprint('Reason: ', e.reason)\n",
    "\t\tpass\n",
    "\texcept socket.timeout as e:\n",
    "\t\t# if urlopen takes too long just skip the url\n",
    "\t\tpass\n",
    "\texcept IncompleteRead:\n",
    "\t\t# Oh well, reconnect and keep trucking\n",
    "\t\tpass\n",
    "\texcept ConnectionResetError:\n",
    "\t\tpass\n",
    "\texcept ssl.SSLWantReadError:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn potential_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to read policy texts from specified directory. The language and length of the policy is held against a check to filter out irrelevant policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(n_words_policy, check_reduced):\n",
    "\t# print(\"Extracting texts from: \" + data_dir)\n",
    "\tall_files = os.listdir(data_dir)\n",
    "\tselected_files = []\n",
    "\tpolicies_list = []\n",
    "\ttitles = []\n",
    "\t\n",
    "\t# filter on txt files (redundant, but safety first)\n",
    "\ttxt_files = filter(lambda x: x[-4:] == '.txt', all_files)\n",
    "\t# print(\"Potential policies found: \" + str(len(all_files)))\n",
    "\t\n",
    "\tfor file in txt_files:\n",
    "\t\t# print(file)\n",
    "\t\tnew_file = []\n",
    "\t\tpotential_policy_text = \"\"\n",
    "\t\twith open((data_dir + \"\\\\\" + file), \"r\", encoding=\"utf8\") as policy:\n",
    "\t\t\tdata = policy.readlines()\n",
    "\t\t\tpolicy_text = list_to_string(data[5:])\n",
    "\t\t\tpolicy_url = list_to_string(data[1])[5:]\n",
    "\t\t\tnew_file[:5] = data[:5]\n",
    "\t\t\tif len(policy_text.split()) > n_words_policy:\n",
    "\t\t\t\tlang = detect(policy_text)\n",
    "\t\t\t\tif lang == 'en':\n",
    "\t\t\t\t\t# print(\"het is engels\")\n",
    "\t\t\t\t\t# print(lang)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# -if text can be reduced, reduce\n",
    "\t\t\t\t\tif check_reduced:\n",
    "\t\t\t\t\t\tpotential_policy_text = find_relevant_paragraph(policy_url)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif len(potential_policy_text) > 20:\n",
    "\t\t\t\t\t\t# print(\"Considering... reduced text\")\n",
    "\t\t\t\t\t\tpolicies_list.append(potential_policy_text)\n",
    "\t\t\t\t\t\tnew_file.append(\"Reduced Policy: \" + potential_policy_text)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# print(\"Considering... full text\")\n",
    "\t\t\t\t\t\t# otherwise consider text in full\n",
    "\t\t\t\t\t\tpolicies_list.append(policy_text)\n",
    "\t\t\t\t\ttitle = (data[0].rstrip(\"\\n\").replace(\"Title: \", \"\"))\n",
    "\t\t\t\t\ttitles.append(title)\n",
    "\t\t\t\t\tselected_files.append(file)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# print(\"het is geen engels\")\n",
    "\t\t\t\t\t# print(lang)\n",
    "\t\t\t\t\tpass\n",
    "\t\tif len(potential_policy_text) > 20:\n",
    "\t\t\twith open((data_dir + \"\\\\\" + file), \"w\", encoding=\"utf8\") as selected_file:\n",
    "\t\t\t\tselected_file.writelines(new_file)\n",
    "\t\n",
    "\tprint(\"Final number of policies after filtering: \" + str(len(policies_list)))\n",
    "\t# return policies_list, titles\n",
    "\treturn selected_files, policies_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to link the read policies to the URLs of the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_policies(url_list, policy_list, policy_list_text):\n",
    "\turl_policy_list = []\n",
    "\tcounter = 0\n",
    "\tfor url in url_list:\n",
    "\t\turl_counter = 0\n",
    "\t\tfor i, policy in enumerate(policy_list, start=0):  # default is zero\n",
    "\t\t\tif url in policy:\n",
    "\t\t\t\tif url_counter == 0:\n",
    "\t\t\t\t\turl_policy_list.append(policy_list_text[i])\n",
    "\t\t\t\t\t# print(url)\n",
    "\t\t\t\t\t# print(policy)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# print(\"Dubbel\")\n",
    "\t\t\t\t\t# if there are multiple policies for one website, select the one that is longer\n",
    "\t\t\t\t\tif len(policy_list_text[i]) > len(url_policy_list[-1]):\n",
    "\t\t\t\t\t\turl_policy_list[-1] = policy_list_text[i]\n",
    "\t\t\t\turl_counter = 1\n",
    "\t\tif url_counter == 0:\n",
    "\t\t\turl_policy_list.append(\"\")\n",
    "\t\n",
    "\treturn url_policy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labelled dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "\tdf_pp_read = pd.read_excel(dir)\n",
    "\t\n",
    "\t# SELECT ONLY URLS THAT ARE EITHER RELEVANT OR NOT RELEVANT WITH EXCLUSION CRITERIA CODE E1 (NOT RELEVANT)\n",
    "\tdf_pp_relevant = df_pp_read[(df_pp_read['included'] == 'y') | (df_pp_read['included'] == 'e1')]\n",
    "\t\n",
    "\tdf_pp_relevant_scoped = df_pp_relevant[\n",
    "\t\t['URL', 'included', 'UR_explicitly_mentioned', 'access', 'rectification', 'erasure', 'restriction',\n",
    "\t\t 'data_portability', 'object', 'automated_decision_making']]\n",
    "\t\n",
    "\tdf_user_rights = df_pp_relevant[['access', 'rectification', 'erasure', 'restriction',\n",
    "\t                                 'data_portability', 'object', 'automated_decision_making']]\n",
    "\t\n",
    "\t# plot_stats(df_user_rights)\n",
    "\t\n",
    "\t# LIST OF URLs\n",
    "\turl_list = df_pp_relevant_scoped['URL'].tolist()\n",
    "\t\n",
    "\treturn df_pp_read, df_pp_relevant, df_pp_relevant_scoped, df_user_rights, url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify document features (BOG approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features2(document):\n",
    "\tdocument_words = set(document)  # faster than list\n",
    "\tfeatures = {}\n",
    "\tfor word in word_features_p:\n",
    "\t\tfeatures['contains({})'.format(word)] = (word in document_words)\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(policies_list):\n",
    "\tcounter = 0\n",
    "\tlemma = WordNetLemmatizer()\n",
    "\t# Remove punctuation\n",
    "\t# print(\"removing punctuation ...\")\n",
    "\tpolicies_list = [re.sub('[,\\.!?&“”():*;\"]', '', x) for x in policies_list]\n",
    "\tpolicies_list = [re.sub('[\\'’/]', '', x) for x in policies_list]\n",
    "\t\n",
    "\t# Convert the titles to lowercase\n",
    "\tpolicies_list = [x.lower() for x in policies_list]\n",
    "\t\n",
    "\t# remove short words\n",
    "\tpolicies_list = [' '.join([w for w in x.split() if len(w) > 3]) for x in policies_list]\n",
    "\t\n",
    "\t# tokenizing\n",
    "\ttokenized_policies = [x.split() for x in policies_list]\n",
    "\t\n",
    "\t# lemmatize\n",
    "\ttokenized_policies = [[lemma.lemmatize(y) for y in x] for x in tokenized_policies]\n",
    "\t\n",
    "\t# stemming\n",
    "\tporter = PorterStemmer()\n",
    "\ttokenized_policies = [[porter.stem(y) for y in x] for x in tokenized_policies]\n",
    "\t\n",
    "\t# remove emailadresses and URLs\n",
    "\t# tokenized_policies = [[y for y in x if(\"@\" not in y)] for x in tokenized_policies]\n",
    "\ttokenized_policies = [[y for y in x if ((\"@\" not in y) and (\"http\" not in y) and\n",
    "\t                                        (\"wwww\" not in y) and (\"com\" not in y))] for x in tokenized_policies]\n",
    "\t\n",
    "\t\n",
    "\t# remove stopwords\n",
    "\ttokenized_policies = [[y for y in x if not y in stop_words] for x in tokenized_policies]\n",
    "\t\n",
    "\t# print(relevant_words_lemmatized)\n",
    "\t# print(tokenized_policies)\n",
    "\t# print(\"length of policy: \" + str(len(tokenized_policies)))\n",
    "\t# print(\"\\n\")\n",
    "\t\n",
    "\tdetokenized_policies = []\n",
    "\tfor i in range(len(policies_list)):\n",
    "\t\tt = ' '.join(tokenized_policies[i])\n",
    "\t\tdetokenized_policies.append(t)\n",
    "\t\t\n",
    "\treturn detokenized_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tpd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)  # print all values of dataframe\n",
    "\tdir = \"data/PP_comparison_classification.xlsx\"\n",
    "\tdata_dir = \"data/privacy_policies\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the separate datasets and link them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tprint(\"Reading data from: \" + dir + \" ...\")\n",
    "\tdf_pp_read, df_pp_relevant, df_pp_relevant_scoped, df_user_rights, url_list = read_data()\n",
    "\t\n",
    "\t# search for corresponding policies\n",
    "\tprint(\"Extracting policy texts from: \" + data_dir + \" ...\")\n",
    "\tpolicy_list, policy_list_text = read_texts(140, False)\n",
    "\t\n",
    "\tprint(\"Link dataset to extracted policies ... \\n\")\n",
    "\turl_list_policies = select_policies(url_list, policy_list, policy_list_text)\n",
    "\turl_list_policies = pre_processing(url_list_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine datasets in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdf_pp_relevant_scoped.insert(2, 'policy', url_list_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print empty policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pp_relevant_scoped[(df_pp_relevant_scoped['policy'] == \"\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tcategories = ['UR_explicitly_mentioned', 'access', 'rectification', 'erasure', 'restriction',\n",
    "\t              'data_portability', 'object', 'automated_decision_making']\n",
    "\t\n",
    "\t# what is the policy distribution?\n",
    "\tdf_pp_filtered = df_pp_relevant_scoped[(df_pp_relevant_scoped['policy'] != \"\")]\n",
    "\tprint(\"Policy distribution after filtering out empty policy texts\")\n",
    "\tprint(\"* Number of policies included with label \\\"y\\\": \" + str(len(df_pp_filtered[(df_pp_filtered['included'] == 'y')])))\n",
    "\tprint(\"* Number of policies excluded with label \\\"e1\\\": \" + str(len(df_pp_filtered[(df_pp_filtered['included'] == 'e1')])))\n",
    "\tprint(\"Total number of policies: \" + str(len(df_pp_filtered)) + \"\\n\")\n",
    "\t\n",
    "\tfor category in categories:\n",
    "\t\tprint(\"Category {}\".format(category))\n",
    "\t\tprint(\"Positive labels: {}\".format(len(df_pp_filtered[(df_pp_filtered[category] == 1)])))\n",
    "\t\tprint(\"Negative labels: {} \\n\".format(len(df_pp_filtered[(df_pp_filtered[category] == 0)])))\n",
    "\n",
    "\tdf_pp = df_pp_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write processed dataframe to file (attempt to save memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-eebbb4bbcdfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/processed_policies.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pp' is not defined"
     ]
    }
   ],
   "source": [
    "df_pp.to_csv('data/processed_policies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp = pd.read_csv(\"data/processed_policies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary consisting of {policy, label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_dict = [(policy_t.split(), label)\n",
    "# \t             for policy_t in df_pp['policy'].to_list()\n",
    "# \t             for label in df_pp['data_portability'].to_list()]\n",
    "\t\n",
    "\n",
    "# # random.shuffle(documents)\n",
    "# random.shuffle(policy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and compute most frequent words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus after preprocessing: 214047\n",
      "Number of unique words after preprocessing: 6165\n"
     ]
    }
   ],
   "source": [
    "\t# BAG OF WORDS APPROACH\n",
    "\t# # To limit the number of features that the classifier needs to process, we begin by constructing a list of the 2000\n",
    "\t# # most frequent words in the overall corpus [1]. We can then define a feature extractor [2] that simply checks whether\n",
    "\t# # each of these words is present in a given document.\n",
    "\ttokenized_policies2 = [x.split() for x in df_pp['policy'].to_list()]\n",
    "\t#flatten\n",
    "\ttokenized_policies_f = [word for policy in tokenized_policies2 for word in policy]\n",
    "\tall_words_p = nltk.FreqDist(w.lower() for w in tokenized_policies_f)\n",
    "# \tall_words_p.most_common(150)\n",
    "\tprint(\"Number of words in the corpus after preprocessing: {}\".format(len(tokenized_policies_f)))\n",
    "\tprint(\"Number of unique words after preprocessing: {}\".format(len(all_words_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract features:  based on the most frequent words list [top-100 due to memory issues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2b3211e9eee3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword_features_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeaturesets_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_features2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolicy_dict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-2b3211e9eee3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword_features_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeaturesets_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_features2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolicy_dict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-ffc9c1c71554>\u001b[0m in \u001b[0;36mdocument_features2\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_features_p\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contains({})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\tword_features_p = list(all_words_p)[:200]\n",
    "\tfeaturesets_p = [(document_features2(d), c) for (d, c) in policy_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore policy length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RTobject =df_pp[df_pp['object'] == 1]['policy'].str.len()\n",
    "sns.distplot(RTobject, label='Right to Object')\n",
    "NRTobject = df_pp[df_pp['object'] == 0]['policy'].str.len()\n",
    "sns.distplot(NRTobject, label='No GDPR-founded Right to Object')\n",
    "plt.title('Distribution by Length')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide data into training and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_p = int(len(featuresets_p) * 0.5)\n",
    "# train_set_p, test_set_p = featuresets_p[size_p:], featuresets_p[:size_p]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_pp['policy'], df_pp['object'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5054283164782868\n",
      "if contains(inform) == False: \n",
      "  if contains(servic) == False: \n",
      "    if contains(manag) == False: return '1'\n",
      "    if contains(manag) == True: return '0'\n",
      "  if contains(servic) == True: return '0'\n",
      "if contains(inform) == True: \n",
      "  if contains(person) == False: \n",
      "    if contains(delet) == False: return '1'\n",
      "    if contains(delet) == True: return '0'\n",
      "  if contains(person) == True: return '0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classifier = nltk.DecisionTreeClassifier.train(train_set_p)\n",
    "# print(nltk.classify.accuracy(classifier, test_set_p))\n",
    "# print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = nltk.classify.accuracy(nltk_model, test_set_p)*100\n",
    "# print(\"{} Accuracy: {}\".format(\"Decision Tree\", accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countvectorizer Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search for best variables (countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'cv__min_df': (0.5, 0.75, 1.0),\n",
    "#     'tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'cv__max_features': (None, 5000, 10000, 50000),\n",
    "    'cv__ngram_range': ((1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3))  # unigrams or bigrams\n",
    "}\n",
    "        \n",
    "grid_search = GridSearchCV(tfidf_pipeline, parameters)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"tf-idf pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train) \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.76\n",
      "CV ROC_AUC Score: 0.76\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.75        25\n",
      "           1       0.74      0.80      0.77        25\n",
      "\n",
      "    accuracy                           0.76        50\n",
      "   macro avg       0.76      0.76      0.76        50\n",
      "weighted avg       0.76      0.76      0.76        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.75        25\n",
      "           1       0.74      0.80      0.77        25\n",
      "\n",
      "    accuracy                           0.76        50\n",
      "   macro avg       0.76      0.76      0.76        50\n",
      "weighted avg       0.76      0.76      0.76        50\n",
      "\n",
      "                  predicted           \n",
      "                     Object not-Object\n",
      "actual Object            18          7\n",
      "       not-Object         5         20\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the vectorizer\n",
    "count_vectorizer = CountVectorizer(analyzer='word', \n",
    "                            ngram_range=(2, 4), max_df=0.7, min_df=1, max_features=None)\n",
    "\n",
    "# fit and transform on it the training features\n",
    "count_vectorizer.fit(X_train)\n",
    "X_train_word_features = count_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = count_vectorizer.transform(X_test)\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion=\"gini\")\n",
    "\n",
    "classifier.fit(X_train_word_features, y_train)\n",
    "y_pred = classifier.predict(test_features)\n",
    "y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('CV Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print(\"CV ROC_AUC Score: {}\\n\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    index = [['actual', 'actual'], ['Object', 'not-Object']],\n",
    "    columns = [['predicted', 'predicted'], ['Object', 'not-Object']])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Countvector Decision-Tree Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "dot_data = export_graphviz(classifier,\n",
    "                                feature_names=count_vectorizer.get_feature_names(),\n",
    "                                out_file=None,\n",
    "                                filled=True,\n",
    "                                rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "colors = ('turquoise', 'orange')\n",
    "edges = collections.defaultdict(list)\n",
    "\n",
    "for edge in graph.get_edge_list():\n",
    "    edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "\n",
    "for edge in edges:\n",
    "    edges[edge].sort()    \n",
    "    for i in range(2):\n",
    "        dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "        dest.set_fillcolor(colors[i])\n",
    "\n",
    "graph.write_png('output/cv-tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search for best variables (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "tf-idf pipeline: ['vect', 'tfidf', 'dt']\n",
      "parameters:\n",
      "{'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__min_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3))}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 292, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1841, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1217, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
      "  File \"c:\\users\\aaberkan\\onedrive - ugent\\scripts\\nlp4re-privacy-policies\\nlp4re-pp-classification\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1088, in _limit_features\n",
      "    raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 403.803s\n",
      "\n",
      "Best score: 0.729\n",
      "Best parameters set:\n",
      "\ttfidf__max_features: 50000\n",
      "\ttfidf__min_df: 0.75\n",
      "\ttfidf__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'tfidf__min_df': (0.5, 0.75, 1.0),\n",
    "#     'tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3))  # unigrams or bigrams\n",
    "}\n",
    "        \n",
    "grid_search = GridSearchCV(tfidf_pipeline, parameters)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"tf-idf pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train) \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.72\n",
      "CV ROC_AUC Score: 0.7120000000000002\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.84      0.75        25\n",
      "           1       0.79      0.60      0.68        25\n",
      "\n",
      "    accuracy                           0.72        50\n",
      "   macro avg       0.73      0.72      0.72        50\n",
      "weighted avg       0.73      0.72      0.72        50\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.84      0.75        25\n",
      "           1       0.79      0.60      0.68        25\n",
      "\n",
      "    accuracy                           0.72        50\n",
      "   macro avg       0.73      0.72      0.72        50\n",
      "weighted avg       0.73      0.72      0.72        50\n",
      "\n",
      "                  predicted           \n",
      "                     Object not-Object\n",
      "actual Object            21          4\n",
      "       not-Object        10         15\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform on it the training features\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train_word_features = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "tdidf_dt_classifier = DecisionTreeClassifier(criterion=\"gini\")\n",
    "\n",
    "tdidf_dt_classifier.fit(X_train_word_features, y_train)\n",
    "y_pred = tdidf_dt_classifier.predict(test_features)\n",
    "y_pred_prob = tdidf_dt_classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('CV Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print(\"CV ROC_AUC Score: {}\\n\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    index = [['actual', 'actual'], ['Object', 'not-Object']],\n",
    "    columns = [['predicted', 'predicted'], ['Object', 'not-Object']])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize TD-IDF Decision-Tree Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize data\n",
    "dot_data = export_graphviz(tdidf_dt_classifier,\n",
    "                                feature_names=word_vectorizer.get_feature_names(),\n",
    "                                out_file=None,\n",
    "                                filled=True,\n",
    "                                rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "colors = ('turquoise', 'orange')\n",
    "edges = collections.defaultdict(list)\n",
    "\n",
    "for edge in graph.get_edge_list():\n",
    "    edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "\n",
    "for edge in edges:\n",
    "    edges[edge].sort()    \n",
    "    for i in range(2):\n",
    "        dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "        dest.set_fillcolor(colors[i])\n",
    "\n",
    "graph.write_png('output/tfidf-tree.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
